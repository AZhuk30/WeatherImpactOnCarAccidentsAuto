name: Daily Data Update Pipeline

on:
  schedule:
    # Runs daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      date_range:
        description: 'Number of days to fetch'
        required: false
        default: '30'

jobs:
  update-data:
    runs-on: ubuntu-latest
    
    steps:
    # 1. Checkout repository
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    # 2. Set up Python
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    # 3. Install dependencies
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    # 4. Create data directories
    - name: Create data directories
      run: |
        mkdir -p data/processed
        mkdir -p data/raw
        mkdir -p data/logs
    
    # 5. Run ETL Pipeline
    - name: Run ETL Pipeline
      run: |
        echo "Running ETL pipeline..."
        python run_pipeline.py --days ${{ github.event.inputs.date_range || '30' }}
    
    # 6. Verify data files were created
    - name: Verify data files
      run: |
        echo "Checking generated files:"
        ls -la data/processed/
        echo ""
        echo "Weather file size:"
        wc -l data/processed/weather_master.csv || echo "Weather file not found"
        echo ""
        echo "Collisions file size:"
        wc -l data/processed/collisions_master.csv || echo "Collisions file not found"
    
# 7. Commit updated data files
- name: Commit updated data
  run: |
    git config --local user.email "action@github.com"
    git config --local user.name "GitHub Action"
    
    # Add the files
    git add data/processed/*.csv
    
    # Check if there are STAGED changes to commit
    if git diff --staged --quiet; then
      echo "No changes to commit"
    else
      git commit -m "ðŸ¤– Auto-update: Data refresh $(date '+%Y-%m-%d %H:%M')"
      git push
    fi
